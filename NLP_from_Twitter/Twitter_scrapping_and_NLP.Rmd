---
title:    "Natural Language Processing in Social Media"
subtitle: "Oxygen ML module for tweets scrapping and NLP"
date:     "August 20th 2020"
urlcolor: gray
output:
  html_document: 
    theme:        united 
    highlight:    pygments
    toc:          TRUE
    toc_float:    TRUE
    code_folding: show
    includes:
    after_body:   footer.html
  pdf_document:   default
  epuRate::epurate:
    number_sections: FALSE
    code_folding:    "show"
    toc:          TRUE 
    word_document:  default
  rmdformats::readthedown:
    toc:          TRUE 
    toc_float:    TRUE     
---

```{js logo-js, echo=FALSE}
$(document).ready(function() {
  $('#header').parent().prepend('<div id=\"logo\"><img src=\"https://drive.google.com/uc?id=1daMjDA9V2WhZUqS4iyatXxBVg7MvJAZt" style=\"position:absolute; top:0; right:0; padding:20px; height:120px\"></div>');
  $('#header').css('margin-right', '120px')
});
```


```{r libraries, message=FALSE, warning=FALSE}
# Sentiment
library(sentimentr)

# Dates
library(lubridate)

# Text cleaning
library(textclean)

# Visualization
library(plotly)
library(viridis)
library(hrbrthemes)
library(magrittr)

# Core
library(tidyverse)
library(tidyquant)

# Using Twitter API
library(rtweet)

# NLP
library(udpipe)

# Computational processes metrics
library(tictoc)
```

# Get the data

```{r set_twitter_tokens}
## store api keys (these are fake example values; replace with your own keys)
api_key <- "OGcti61Wu8coB61aiWvYmMpsT"
api_secret_key <- "bH9MMduzgbeHjCka0yhHAmV7OcP5GQgM3URKMu4UsXiMl83TC1"

## authenticate via web browser
token <- create_token(
  app = "ucm-ds-test",
  consumer_key = api_key,
  consumer_secret = api_secret_key)

```

```{r retrieve_data}
## search for 500 tweets using hashtag
twts <- search_tweets(q = "#blacklivesmatter", parse = T, n = 1000, lang = "en")
```

```{r create_data_tibble}
tweet_table_data <- twts %>% select(user_id, status_id, dateTime = created_at, 
                                    user = screen_name, text, likes = favorite_count, 
                                    rt = retweet_count)

tweet_table_data
```
# Clean the data
It's important to remove hashtags, tags, urls, html code and other text features. The textclean library is quite easy to use, and also faster than other text replacement functions, for example gsub, but only in big chunks of texts. For tweets, gsub performs better than textclean, but is more difficult to get the appropiate regular expressions for all the changes.

```{r clean_texts_gsub}

tic()
tweet_table_data$text_clean <- gsub("&amp", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("&amp", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("@\\w+", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("[[:punct:]]", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("[[:digit:]]", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("http\\w+", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("[ \t]{2,}", "", tweet_table_data$text)
tweet_table_data$text_clean <- gsub("^\\s+|\\s+$", "", tweet_table_data$text)

tweet_table_data$text_clean <- tolower(tweet_table_data$text_clean)
tweet_table_data$text_clean <- iconv(tweet_table_data$text_clean, "UTF-8", "ASCII", sub="")

toc()

```


```{r clean_text_dplyr_1}
# Just checking dplyr + textclean performance:
tic()
tweet_table_data %>% mutate(text_clean = text %>%
                              replace_tag() %>%
                              replace_emoji() %>%
                              replace_non_ascii() %>%
                              replace_number() %>%
                              replace_url() %>%
                              replace_html() %>%
                              tolower()
                            ) 
toc()
```



```{r}
tweet_table_data <- tweet_table_data %>% mutate(text_clean = text) %>%
  replace_hash(text_clean) %>%
  replace_tag(text_clean) %>%
  replace_emoji(text_clean) %>%
  replace_non_ascii(text_clean) %>%
  replace_number(text_clean) %>%
  replace_url(text_clean) %>%
  replace_html(text_clean) %>%
  tolower(text_clean)
```


# Get sentiment

```{r get_sentiment}
tweet_sentiments <- tweet_table_data %>% mutate(text_split = get_sentences(text_clean)) %$% sentiment_by(text_split, list(status_id, user, dateTime, text, text_clean, rt))

tweet_sentiments <- tweet_sentiments %>% 
  mutate(minutes_from_now = round(difftime(tweet_sentiments$dateTime, 
                                           now(tzone = "UTC"), units="mins"),0))

tweet_sentiments
```

# Plot tweet sentiment average in time

```{r plot_sentiment}
label_wrap <- label_wrap_gen(width = 60)
g <- tweet_sentiments %>% 
  ggplot(aes(x=dateTime, y=ave_sentiment, size = rt, color = ave_sentiment)) +
    geom_point(alpha=0.9,
               aes(text = str_glue("<b>Date-time:,</b> {dateTime}
                                      <b>Sentiment score:</b> {round(ave_sentiment,2)}
                                      <b>Retweets:</b> {rt}
                                      <b>Tweet:</b> {label_wrap(text)}"))) +
    geom_hline(aes(yintercept = mean(ave_sentiment)), color = "black") +
    geom_hline(aes(yintercept = median(ave_sentiment) + 1.96*IQR(ave_sentiment)), color = "#ffd633") +
    geom_hline(aes(yintercept = median(ave_sentiment) - 1.96*IQR(ave_sentiment)), color = "#600080") +
    scale_size(range = c(.2, 18)) +
    scale_color_viridis(option="plasma") +
    theme_ipsum() +
    theme(legend.position = "none") +
    ggtitle("Average sentiment of tweets in time") +
    ylab("Average sentiment of tweet") +
    xlab("Date - time")

ggplotly(g, tooltip = "text") 
```

# Table of most RT tweets
```{r table_most_rt}

top_positive_rt <- tweet_sentiments %>% 
  arrange(desc(rt)) %>% mutate(sentiment_score = round(ave_sentiment,2)) %>% select(rt, text, sentiment_score)

top_positive_rt
```

# Get tweets polarity

```{r}
polarity_tweets <- tweet_sentiments %>% 
  mutate(polarity_level = factor(ifelse(ave_sentiment < 0, "Negative",
                                 ifelse(ave_sentiment > 0, "Positive","Neutral"))))

polarity_tweets <- polarity_tweets %>% group_by(minutes_from_now, polarity_level) %>% mutate(count = n())
```



# Plot tweets polarity


```{r plot_polarity_animated}
g <- polarity_tweets %>% ggplot(aes(x=minutes_from_now, y=count, group=polarity_level, color=polarity_level)) +
    geom_line() +
    geom_point() +
    ggtitle("Evolution of tweets per polarity") +
    scale_color_viridis(discrete = T, option="plasma") +
    theme_ipsum() +
    ylab("Number of tweets")
    #+ transition_reveal(minutes_from_now)

g
```


# Get emotions

```{r get_emotions}
emotion_tweets <- tweet_table_data %>% get_sentences(tweet_table_data$text_clean) %>% emotion(drop.unused.emotions = TRUE)

```
# Plot emotions in time

```{r plot_emotions}
emotion_tweets_ordered <- arrange(emotion_tweets, dateTime)
g <- plot(emotion_tweets_ordered, facet = FALSE)
ggplotly(g, tooltip = c("emotion"))
```



# NLP

```{r import_model}
ud_model <- udpipe_download_model(language = "english")
saveRDS(ud_model, "ud_model.rds")
ud_model <- udpipe_load_model(ud_model$file_model)

```

```{r annotate_texts}
x <- udpipe_annotate(ud_model, x = tweet_sentiments$text_clean, doc_id = tweet_sentiments$status_id)
x <- as.data.frame(x)
str(x)
```

```{r plot_common_adjectives}
# Prepare data:
stats <- subset(x, upos %in% c("ADJ")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))

# Plot
g <- ggplot(stats[1:30,], aes(x=key, y=freq)) +
    geom_bar(stat="identity", fill="#69b3a2", alpha=.6, width=.4) +
    coord_flip() +
    xlab("") +
    theme_bw() + 
    ggtitle("Top 30 most frequent adjectives in tweets")

ggplotly(g, tooltip = c("key", "freq"))
```

```{r cooc_table}
cooc <- cooccurrence(x = subset(x, upos %in% c('NOUN', 'ADJ')), 
                    term  = 'lemma', 
                    group = c('doc_id', 'paragraph_id', 'sentence_id'))
```

```{r coocurrences_dynamic}
library(networkD3)
grapho <- data.frame(from=cooc$term1[1:100], to=cooc$term2[1:100])
p <- simpleNetwork(grapho, 
                   height="500px", 
                   width="700px",
                   linkDistance = 20,
                   charge = -100,
                   fontSize = 14,
                   fontFamily = "arial",
                   zoom = T)
p
```

```{r}

g <- polarity_tweets %>% filter(polarity_level != 'Neutral') %>%
  ggplot( aes(x=polarity_level, y=ave_sentiment, fill=polarity_level)) + 
    geom_violin() +
  scale_fill_viridis(discrete=T, option="plasma") +
    theme_ipsum() +
    theme(legend.position = "none") +
    ggtitle("How positive or negative are the tweets. A violin plot") +
    ylab("Average sentiment of tweets") +
    xlab("Tweets polarity category")

ggplotly(g, tooltip = c("polarity_level", "ave_sentiment"))

```




